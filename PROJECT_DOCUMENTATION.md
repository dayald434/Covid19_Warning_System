# ü¶† COVID-19 Early Warning System - Complete Project Documentation

## Table of Contents
1. [Executive Summary](#executive-summary)
2. [Project Overview](#project-overview)
3. [System Architecture](#system-architecture)
4. [Data Sources & Description](#data-sources--description)
5. [Data Preparation Pipeline](#data-preparation-pipeline)
6. [Feature Engineering](#feature-engineering)
7. [Model Development](#model-development)
8. [Model Performance](#model-performance)
9. [Deployment & Usage](#deployment--usage)
10. [Installation Guide](#installation-guide)
11. [API Reference](#api-reference)
12. [Testing](#testing)
13. [Project Structure](#project-structure)
14. [Future Enhancements](#future-enhancements)
15. [References](#references)

---

## Executive Summary

### Project Goal
Predict the required level of public health interventions **7 days in advance** using current COVID-19 epidemiological indicators, providing policymakers with early warning to prepare and respond effectively.

### Key Achievements
- ‚úÖ **99.5% accuracy** in predicting critical lockdown scenarios
- ‚úÖ **99.3% overall model accuracy**
- ‚úÖ Trained on **51,896 samples** from **201 countries** (2020-2023)
- ‚úÖ **34 engineered features** across 5 feature categories
- ‚úÖ Interactive web interface for real-time predictions
- ‚úÖ 7-day advance warning for policy planning

### Business Impact
- Early warning enables proactive resource allocation
- Reduces decision-making time from days to minutes
- Evidence-based policy recommendations
- Prevents healthcare system overload through early intervention

---

## Project Overview

### Problem Statement
During the COVID-19 pandemic, policymakers faced critical decisions about implementing public health measures (lockdowns, restrictions, monitoring). However, these decisions were often reactive, responding to already-critical situations rather than preventing them.

### Solution
A machine learning system that analyzes **current** COVID-19 indicators and predicts the **required intervention level 7 days ahead**, giving authorities time to:
- Prepare healthcare capacity
- Communicate with the public
- Mobilize resources
- Implement preventive measures

### What This System Does NOT Do
- ‚ùå Does NOT predict future case numbers
- ‚ùå Does NOT forecast outbreak occurrence
- ‚ùå Does NOT predict individual infection risk

### What This System DOES
- ‚úÖ Predicts **action recommendations** based on current trends
- ‚úÖ Classifies situations into 4 warning levels
- ‚úÖ Provides 7-day planning horizon
- ‚úÖ Supports evidence-based decision making

---

## System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DATA SOURCES                            ‚îÇ
‚îÇ  Johns Hopkins CSSE COVID-19 Time Series Data              ‚îÇ
‚îÇ  - Confirmed Cases                                          ‚îÇ
‚îÇ  - Deaths                                                   ‚îÇ
‚îÇ  - Recovered (deprecated 2023)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATA PREPARATION PIPELINE                      ‚îÇ
‚îÇ  Step 1: Data Integration (Wide ‚Üí Long Format)             ‚îÇ
‚îÇ  Step 2: Data Cleaning & Quality Control                   ‚îÇ
‚îÇ  Step 3: Feature Engineering (40+ features)                ‚îÇ
‚îÇ  Step 4: Population Normalization                          ‚îÇ
‚îÇ  Step 5: Target Variable Creation (7-day ahead)            ‚îÇ
‚îÇ  Step 6: Data Export                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                MODEL TRAINING PIPELINE                      ‚îÇ
‚îÇ  - Random Forest Classifier                                 ‚îÇ
‚îÇ  - 100 estimators, depth=10                                ‚îÇ
‚îÇ  - Balanced class weights                                   ‚îÇ
‚îÇ  - 80/20 train-test split                                  ‚îÇ
‚îÇ  - Stratified sampling                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  TRAINED MODEL ARTIFACT                     ‚îÇ
‚îÇ  - best_covid_warning_model.pkl (7.7 MB)                   ‚îÇ
‚îÇ  - model_metadata.pkl                                       ‚îÇ
‚îÇ  - per_class_performance.csv                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              STREAMLIT WEB APPLICATION                      ‚îÇ
‚îÇ  - Single Prediction Interface                             ‚îÇ
‚îÇ  - Batch Upload & Analysis                                 ‚îÇ
‚îÇ  - Feature Importance Visualization                        ‚îÇ
‚îÇ  - Interactive Testing with Presets                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Language** | Python | 3.8+ |
| **Data Processing** | Pandas | 2.0.0+ |
| **Numerical Computing** | NumPy | 1.24.0+ |
| **Machine Learning** | Scikit-learn | 1.3.0+ |
| **Model Persistence** | Joblib | 1.3.0+ |
| **Web Framework** | Streamlit | 1.28.0+ |
| **Visualization** | Matplotlib, Seaborn | 3.7.0+, 0.12.0+ |

---

## Data Sources & Description

### Primary Data Source
**Johns Hopkins University CSSE COVID-19 Data Repository**
- Source: https://github.com/CSSEGISandData/COVID-19
- Format: Time series CSV files
- Granularity: Country and Province/State level
- Update Frequency: Daily (historical data 2020-2023)

### Data Files

#### 1. time_series_covid19_confirmed_global.csv
- **Purpose**: Cumulative confirmed COVID-19 cases
- **Structure**: 
  - Rows: 289 locations (countries/provinces)
  - Columns: 1,147 (4 metadata + 1,143 daily columns)
  - Date Range: January 22, 2020 to March 9, 2023
- **Key Fields**:
  - `Province/State`: Sub-national region
  - `Country/Region`: Country name
  - `Lat`, `Long`: Geographic coordinates
  - Daily columns: `1/22/20`, `1/23/20`, ..., `3/9/23`

#### 2. time_series_covid19_deaths_global.csv
- **Purpose**: Cumulative COVID-19 deaths
- **Structure**: Same as confirmed cases
- **Date Range**: January 22, 2020 to March 9, 2023

#### 3. time_series_covid19_recovered_global.csv
- **Purpose**: Cumulative recovered cases (discontinued)
- **Structure**: 274 locations √ó 1,147 columns
- **Note**: Deprecated by Johns Hopkins in 2023; many missing values

### Data Coverage

| Metric | Value |
|--------|-------|
| **Countries Covered** | 201 |
| **Time Period** | 1,143 days (3+ years) |
| **Total Records** | 337,185 country-date observations |
| **Training Samples** | 51,896 (after cleaning) |
| **Geographic Coverage** | Global (all continents) |

### Population Data
- **Source**: World Bank 2020 population estimates
- **Coverage**: 70+ countries with explicit mappings
- **Missing Data Handling**: Median imputation (170,307 values filled)

---

## Data Preparation Pipeline

### Pipeline Overview
The data preparation pipeline transforms raw COVID-19 time series data into a feature-rich dataset ready for machine learning. It consists of 6 major steps:

### Step 1: Data Integration

**Objective**: Convert wide-format time series to long-format relational data

**Process**:
```python
# Input: Wide format (289 rows √ó 1,147 columns)
# Output: Long format (337,185 rows √ó 8 columns)

1. Load three datasets (Confirmed, Deaths, Recovered)
2. Melt each from wide to long format:
   - ID variables: Province/State, Country/Region, Lat, Long
   - Value variables: All date columns
   - Creates: Date, Value pairs
3. Merge datasets on location + date keys
4. Parse dates to datetime objects
```

**Output Schema**:
- `Province/State`: String (filled "All" for country-level)
- `Country/Region`: String
- `Lat`, `Long`: Float (geographic coordinates)
- `Date`: Datetime
- `Confirmed`: Integer (cumulative cases)
- `Deaths`: Integer (cumulative deaths)
- `Recovered`: Integer (cumulative recovered)

---

### Step 2: Data Cleaning

**2.1 Missing Value Handling**
```python
- Confirmed, Deaths, Recovered: Fill with 0
- Lat/Long: Fill with country centroid
- Province/State: Fill with "All"
```

**2.2 Monotonicity Enforcement**
Cumulative values must never decrease:
```python
df[['Confirmed', 'Deaths', 'Recovered']] = 
    df.groupby(['Country/Region', 'Province/State'])
      [['Confirmed', 'Deaths', 'Recovered']].cummax()
```

**2.3 Daily Change Calculation**
```python
Daily_Cases = Confirmed.diff() per group
Daily_Deaths = Deaths.diff() per group
Daily_Recovered = Recovered.diff() per group
```

**2.4 Negative Value Handling**
```python
# Data corrections can cause negative daily values
Daily_Cases = max(Daily_Cases, 0)
Daily_Deaths = max(Daily_Deaths, 0)
```

**2.5 Outlier Detection & Capping**
```python
# Cap at 99th percentile per country/province group
threshold = group['Daily_Cases'].quantile(0.99)
Daily_Cases = Daily_Cases.clip(upper=threshold)
```

**2.6 Smoothing (7-day Moving Average)**
```python
Cases_7d_MA = Daily_Cases.rolling(window=7).mean()
Deaths_7d_MA = Daily_Deaths.rolling(window=7).mean()
```

---

### Step 3: Feature Engineering

Comprehensive feature creation across 5 categories:

#### 3.1 Temporal Features (8 features)

| Feature | Description | Formula |
|---------|-------------|---------|
| `DayOfWeek` | Day of week (0-6) | `Date.dayofweek` |
| `Month` | Month (1-12) | `Date.month` |
| `Quarter` | Quarter (1-4) | `Date.quarter` |
| `Year` | Year | `Date.year` |
| `IsWeekend` | Weekend flag | `1 if DayOfWeek in [5,6] else 0` |
| `Days_Since_Start` | Days since pandemic start | `Date - 2020-01-22` |
| `Days_Since_100` | Days since 100th case | `Date - FirstDate(Confirmed>=100)` |

**Purpose**: Capture seasonality, reporting patterns, and outbreak maturity

#### 3.2 Growth Metrics (7 features)

| Feature | Description | Formula |
|---------|-------------|---------|
| `Growth_Rate` | Daily case growth rate | `Daily_Cases[t] / Daily_Cases[t-1] - 1` |
| `Death_Growth` | Daily death growth rate | `Daily_Deaths[t] / Daily_Deaths[t-1] - 1` |
| `Acceleration` | Change in growth rate | `Growth_Rate[t] - Growth_Rate[t-1]` |
| `Doubling_Time` | Days to double cases | `log(2) / log(1 + Growth_Rate)` |
| `Log_Cases` | Log-transformed cases | `log(1 + Daily_Cases)` |
| `Log_Deaths` | Log-transformed deaths | `log(1 + Daily_Deaths)` |

**Purpose**: Measure outbreak velocity and trajectory

**Special Handling**:
```python
# Safe growth rate calculation (avoid noise)
def safe_growth_rate(series, threshold=50):
    series[series < threshold] = NaN
    return series.pct_change()
```

#### 3.3 Severity Metrics (4 features)

| Feature | Description | Formula |
|---------|-------------|---------|
| `CFR` | Case Fatality Rate | `(Deaths / Confirmed) √ó 100` |
| `Active_Cases` | Currently infected | `Confirmed - Deaths - Recovered` |
| `Recovery_Rate` | Recovery proportion | `Recovered / Confirmed` |
| `Death_to_Case_Ratio` | Daily death-to-case ratio | `Daily_Deaths / Daily_Cases` |

**Purpose**: Assess outbreak severity and healthcare burden

#### 3.4 Population-Normalized Metrics (2 features)

| Feature | Description | Formula |
|---------|-------------|---------|
| `Cases_per_100k` | Cases per 100,000 population | `(Confirmed / Population) √ó 100,000` |
| `Deaths_per_100k` | Deaths per 100,000 population | `(Deaths / Population) √ó 100,000` |

**Purpose**: Enable fair comparison across countries of different sizes

#### 3.5 Intervention Indicators (4 features)

| Feature | Description | Values |
|---------|-------------|--------|
| `NPI_Phase` | Non-pharmaceutical intervention phase | Pre-intervention, Lockdown, Reopening, Post-reopening |
| `Vaccine_Period` | Vaccine availability | Pre-vaccine, Post-vaccine |
| `Is_Lockdown` | Binary lockdown flag | 0 or 1 |
| `Is_Post_Vaccine` | Binary vaccine flag | 0 or 1 |

**NPI Phase Definitions**:
```python
Pre-intervention: 2020-01-22 to 2020-03-15  (Early pandemic)
Lockdown:        2020-03-16 to 2020-06-01  (Global lockdowns)
Reopening:       2020-06-02 to 2020-12-01  (Gradual reopening)
Post-reopening:  2020-12-02 to 2023-03-09  (Living with COVID)
```

**Vaccine Period**:
```python
Pre-vaccine:  Before 2021-01-01
Post-vaccine: From 2021-01-01 onwards
```

---

### Step 4: Population Normalization

**Process**:
1. Map countries to 2020 population estimates
2. Fill missing populations with median (73,449,178)
3. Calculate per-capita metrics

**Population Data Coverage**: 70+ countries explicitly mapped

---

### Step 5: Target Variable Creation (7-Day Ahead)

This is the **critical innovation** of the project - creating a forward-looking target variable.

#### Concept
Instead of predicting future case numbers, we predict what **intervention level** will be needed in 7 days based on current trends.

#### Process

**Step 5.1**: Shift key metrics 7 days into the future
```python
Growth_Rate_future7d = Growth_Rate.shift(-7)
Cases_per_100k_future7d = Cases_per_100k.shift(-7)
Doubling_Time_future7d = Doubling_Time.shift(-7)
CFR_future7d = CFR.shift(-7)
```

**Step 5.2**: Classify future situation into 4 warning levels

#### Warning Level Classification Algorithm

```python
def assign_warning_level(growth, cases_100k, doubling, cfr):
    risk_score = 0
    
    # 1. Growth Rate (40% weight)
    if growth > 0.20:    risk_score += 4
    elif growth > 0.10:  risk_score += 3
    elif growth > 0.05:  risk_score += 2
    elif growth > 0:     risk_score += 1
    
    # 2. Disease Burden (30% weight)
    if cases_100k > 1000:   risk_score += 4
    elif cases_100k > 500:  risk_score += 3
    elif cases_100k > 200:  risk_score += 2
    elif cases_100k > 50:   risk_score += 1
    
    # 3. Doubling Time (20% weight)
    if 0 < doubling < 7:    risk_score += 3
    elif doubling < 14:     risk_score += 2
    elif doubling < 30:     risk_score += 1
    
    # 4. Case Fatality Rate (10% weight)
    if cfr > 5:      risk_score += 2
    elif cfr > 3:    risk_score += 1
    
    # Classification
    if risk_score >= 10:   return 'CRITICAL_LOCKDOWN'
    elif risk_score >= 6:  return 'HIGH_RESTRICTIONS'
    elif risk_score >= 3:  return 'MODERATE_MEASURES'
    else:                  return 'LOW_MONITORING'
```

#### Warning Levels Explained

| Level | Risk Score | Recommended Actions | Example Scenario |
|-------|-----------|---------------------|------------------|
| **CRITICAL_LOCKDOWN** | 10-13 | ‚Ä¢ Full lockdown<br>‚Ä¢ Non-essential business closure<br>‚Ä¢ Stay-at-home orders<br>‚Ä¢ Emergency healthcare measures | Explosive growth (>20%/day)<br>High burden (>1000/100k)<br>Rapid doubling (<7 days) |
| **HIGH_RESTRICTIONS** | 6-9 | ‚Ä¢ Partial lockdown<br>‚Ä¢ Capacity limits<br>‚Ä¢ Remote work mandates<br>‚Ä¢ Enhanced contact tracing | Sustained growth (10-20%/day)<br>Moderate burden (500-1000/100k)<br>Medium doubling (7-14 days) |
| **MODERATE_MEASURES** | 3-5 | ‚Ä¢ Mask mandates<br>‚Ä¢ Social distancing<br>‚Ä¢ Event restrictions<br>‚Ä¢ Testing expansion | Slow growth (5-10%/day)<br>Emerging burden (200-500/100k)<br>Slow doubling (14-30 days) |
| **LOW_MONITORING** | 0-2 | ‚Ä¢ Enhanced surveillance<br>‚Ä¢ Voluntary precautions<br>‚Ä¢ Public awareness<br>‚Ä¢ Preparedness | Minimal growth (<5%/day)<br>Low burden (<200/100k)<br>Very slow/no doubling |

#### Target Variable Distribution

From the actual dataset:
```
HIGH_RESTRICTIONS:   23,802 samples (45.9%)  <- Most common
CRITICAL_LOCKDOWN:   20,424 samples (39.4%)
MODERATE_MEASURES:    6,572 samples (12.7%)
LOW_MONITORING:       1,098 samples ( 2.1%)  <- Rarest (class imbalance)
```

---

### Step 6: Data Export

**Output File**: `data/processed/covid19_prepared_data.csv`

**Size**: 116 MB (337,185 rows √ó 42 columns)

**Final Schema** (42 columns):
- Identifiers (4): Province/State, Country/Region, Lat, Long
- Temporal (2): Date, Year
- Raw counts (6): Confirmed, Deaths, Recovered, Daily_Cases, Daily_Deaths, Daily_Recovered
- Smoothed (2): Cases_7d_MA, Deaths_7d_MA
- Temporal features (8): DayOfWeek, Month, Quarter, IsWeekend, Days_Since_Start, Days_Since_100
- Growth metrics (7): Growth_Rate, Death_Growth, Acceleration, Doubling_Time, Log_Cases, Log_Deaths
- Severity metrics (4): CFR, Active_Cases, Recovery_Rate, Death_to_Case_Ratio
- Population (3): Population, Cases_per_100k, Deaths_per_100k
- Intervention (4): NPI_Phase, Vaccine_Period, Is_Lockdown, Is_Post_Vaccine
- Future metrics (4): *_future7d features
- **Target (1): Warning_Level_7d_Ahead**

---

## Feature Engineering

### Feature Transformation Lineage

This section shows how **input features** (from raw data) are transformed into **output features** (used for training).

#### Input Features from Raw Data (8 columns)

| Input Feature | Type | Source | Description |
|---------------|------|--------|-------------|
| `Province/State` | String | Raw CSV | Sub-national region (nullable) |
| `Country/Region` | String | Raw CSV | Country name |
| `Lat` | Float | Raw CSV | Latitude coordinate |
| `Long` | Float | Raw CSV | Longitude coordinate |
| `Date` | String ‚Üí Datetime | Raw CSV (transformed) | Date column (1/22/20 format) |
| `Confirmed` | Integer | Raw CSV (melted) | Cumulative confirmed cases |
| `Deaths` | Integer | Raw CSV (melted) | Cumulative deaths |
| `Recovered` | Integer | Raw CSV (melted) | Cumulative recovered cases |

---

### Output Features Created (42 total columns)

#### Category 1: Preserved Input Features (4 features)
*Direct pass-through from raw data*

| Output Feature | Input Source | Transformation |
|----------------|-------------|----------------|
| `Province/State` | `Province/State` | Fill NaN with "All" |
| `Country/Region` | `Country/Region` | No change |
| `Lat` | `Lat` | Fill NaN with country centroid |
| `Long` | `Long` | Fill NaN with country centroid |

---

#### Category 2: Temporal Features (8 features)
*Extracted from Date column*

| Output Feature | Input Source | Formula/Transformation |
|----------------|-------------|------------------------|
| `Date` | `Date` (string) | `pd.to_datetime(Date)` |
| `DayOfWeek` | `Date` | `Date.dt.dayofweek` (0=Monday, 6=Sunday) |
| `Month` | `Date` | `Date.dt.month` (1-12) |
| `Quarter` | `Date` | `Date.dt.quarter` (1-4) |
| `Year` | `Date` | `Date.dt.year` |
| `IsWeekend` | `DayOfWeek` | `1 if DayOfWeek in [5,6] else 0` |
| `Days_Since_Start` | `Date` | `(Date - 2020-01-22).days` |
| `Days_Since_100` | `Date`, `Confirmed` | Days since first date where Confirmed ‚â• 100 |

---

#### Category 3: Daily Change Features (3 features)
*Derived from cumulative counts*

| Output Feature | Input Source | Formula/Transformation |
|----------------|-------------|------------------------|
| `Daily_Cases` | `Confirmed` | `Confirmed.diff()` per country/province group |
| `Daily_Deaths` | `Deaths` | `Deaths.diff()` per country/province group |
| `Daily_Recovered` | `Recovered` | `Recovered.diff()` per country/province group |

**Processing Steps:**
1. Group by `[Country/Region, Province/State]`
2. Calculate difference from previous day
3. Fill first row with 0
4. Cap negative values to 0 (data corrections)
5. Cap outliers at 99th percentile per group

---

#### Category 4: Smoothed Features (2 features)
*7-day moving average for noise reduction*

| Output Feature | Input Source | Formula/Transformation |
|----------------|-------------|------------------------|
| `Cases_7d_MA` | `Daily_Cases` | `Daily_Cases.rolling(window=7).mean()` |
| `Deaths_7d_MA` | `Daily_Deaths` | `Daily_Deaths.rolling(window=7).mean()` |

---

#### Category 5: Growth Metrics (7 features)
*Measuring outbreak velocity*

| Output Feature | Input Sources | Formula/Transformation |
|----------------|--------------|------------------------|
| `Growth_Rate` | `Daily_Cases` | `pct_change()` with threshold ‚â•50 cases |
| `Death_Growth` | `Daily_Deaths` | `pct_change()` with threshold ‚â•10 deaths |
| `Acceleration` | `Growth_Rate` | `Growth_Rate.diff()` |
| `Doubling_Time` | `Growth_Rate` | `log(2) / log(1 + Growth_Rate)` |
| `Log_Cases` | `Daily_Cases` | `log(1 + Daily_Cases)` |
| `Log_Deaths` | `Daily_Deaths` | `log(1 + Daily_Deaths)` |

**Special Processing:**
```python
# Safe growth rate calculation
def safe_growth_rate(series, threshold=50):
    clean_series = series.copy()
    clean_series[clean_series < threshold] = NaN
    return clean_series.pct_change()
```

---

#### Category 6: Severity Metrics (4 features)
*Healthcare burden indicators*

| Output Feature | Input Sources | Formula/Transformation |
|----------------|--------------|------------------------|
| `CFR` | `Deaths`, `Confirmed` | `(Deaths / Confirmed) √ó 100` |
| `Active_Cases` | `Confirmed`, `Deaths`, `Recovered` | `Confirmed - Deaths - Recovered` |
| `Recovery_Rate` | `Recovered`, `Confirmed` | `Recovered / Confirmed` |
| `Death_to_Case_Ratio` | `Daily_Deaths`, `Daily_Cases` | `Daily_Deaths / Daily_Cases` |

---

#### Category 7: Population-Normalized Metrics (3 features)
*Per-capita comparisons*

| Output Feature | Input Sources | Formula/Transformation |
|----------------|--------------|------------------------|
| `Population` | `Country/Region` + External Data | Map from `POPULATION_DATA` dict ‚Üí Fill NaN with median |
| `Cases_per_100k` | `Confirmed`, `Population` | `(Confirmed / Population) √ó 100,000` |
| `Deaths_per_100k` | `Deaths`, `Population` | `(Deaths / Population) √ó 100,000` |

**Population Data Source:** World Bank 2020 estimates (70+ countries mapped)

---

#### Category 8: Intervention Indicators (4 features)
*Policy context markers*

| Output Feature | Input Source | Formula/Transformation |
|----------------|-------------|------------------------|
| `NPI_Phase` | `Date` | Assigned based on date ranges:<br>‚Ä¢ 2020-01-22 to 2020-03-15: Pre-intervention<br>‚Ä¢ 2020-03-16 to 2020-06-01: Lockdown<br>‚Ä¢ 2020-06-02 to 2020-12-01: Reopening<br>‚Ä¢ 2020-12-02 to 2023-03-09: Post-reopening |
| `Vaccine_Period` | `Date` | `'Post-vaccine' if Date >= 2021-01-01 else 'Pre-vaccine'` |
| `Is_Lockdown` | `NPI_Phase` | `1 if NPI_Phase == 'Lockdown' else 0` |
| `Is_Post_Vaccine` | `Vaccine_Period` | `1 if Vaccine_Period == 'Post-vaccine' else 0` |

---

#### Category 9: Future Shifted Features (4 features)
*7-day ahead versions for target creation*

| Output Feature | Input Source | Formula/Transformation |
|----------------|-------------|------------------------|
| `Growth_Rate_future7d` | `Growth_Rate` | `shift(-7)` per group |
| `Cases_per_100k_future7d` | `Cases_per_100k` | `shift(-7)` per group |
| `Doubling_Time_future7d` | `Doubling_Time` | `shift(-7)` per group |
| `CFR_future7d` | `CFR` | `shift(-7)` per group |

**Purpose:** These are NOT used as training features. They're intermediate values used to create the target variable.

---

#### Category 10: Target Variable (1 feature)
*The prediction target*

| Output Feature | Input Sources | Formula/Transformation |
|----------------|--------------|------------------------|
| `Warning_Level_7d_Ahead` | `Growth_Rate_future7d`<br>`Cases_per_100k_future7d`<br>`Doubling_Time_future7d`<br>`CFR_future7d` | Risk score algorithm:<br>1. Growth rate ‚Üí 0-4 points<br>2. Disease burden ‚Üí 0-4 points<br>3. Doubling time ‚Üí 0-3 points<br>4. CFR ‚Üí 0-2 points<br><br>Total score ‚Üí Classification:<br>‚Ä¢ 10-13: CRITICAL_LOCKDOWN<br>‚Ä¢ 6-9: HIGH_RESTRICTIONS<br>‚Ä¢ 3-5: MODERATE_MEASURES<br>‚Ä¢ 0-2: LOW_MONITORING |

---

### Complete Feature Transformation Flow

```
RAW INPUT (8 columns)
‚îú‚îÄ Province/State
‚îú‚îÄ Country/Region  
‚îú‚îÄ Lat
‚îú‚îÄ Long
‚îú‚îÄ Date (string)
‚îú‚îÄ Confirmed (wide format)
‚îú‚îÄ Deaths (wide format)
‚îî‚îÄ Recovered (wide format)

          ‚Üì [Melt Wide ‚Üí Long]
          ‚Üì [Parse Dates]
          ‚Üì [Fill Missing Values]
          
INTERMEDIATE (8 columns)
‚îú‚îÄ Province/State (filled)
‚îú‚îÄ Country/Region
‚îú‚îÄ Lat (filled)
‚îú‚îÄ Long (filled)
‚îú‚îÄ Date (datetime)
‚îú‚îÄ Confirmed (long format)
‚îú‚îÄ Deaths (long format)
‚îî‚îÄ Recovered (long format)

          ‚Üì [Feature Engineering Steps]
          
ENGINEERED FEATURES (34 new columns)
‚îú‚îÄ Temporal (8): DayOfWeek, Month, Quarter, Year, IsWeekend, Days_Since_Start, Days_Since_100
‚îú‚îÄ Daily Changes (3): Daily_Cases, Daily_Deaths, Daily_Recovered
‚îú‚îÄ Smoothed (2): Cases_7d_MA, Deaths_7d_MA
‚îú‚îÄ Growth (7): Growth_Rate, Death_Growth, Acceleration, Doubling_Time, Log_Cases, Log_Deaths
‚îú‚îÄ Severity (4): CFR, Active_Cases, Recovery_Rate, Death_to_Case_Ratio
‚îú‚îÄ Population (3): Population, Cases_per_100k, Deaths_per_100k
‚îú‚îÄ Intervention (4): NPI_Phase, Vaccine_Period, Is_Lockdown, Is_Post_Vaccine
‚îî‚îÄ Future Shifted (4): *_future7d versions

          ‚Üì [Target Creation]
          
TARGET VARIABLE (1 column)
‚îî‚îÄ Warning_Level_7d_Ahead

          ‚Üì [Final Dataset]
          
FINAL OUTPUT (42 columns total)
‚îú‚îÄ Metadata (4): Province/State, Country/Region, Lat, Long
‚îú‚îÄ Temporal (2): Date, Year
‚îú‚îÄ Cumulative (3): Confirmed, Deaths, Recovered
‚îú‚îÄ Features for ML (34): All engineered features except future7d
‚îî‚îÄ Target (1): Warning_Level_7d_Ahead
```

---

### Features Used for Training vs. Features in Dataset

**Total Features in Dataset:** 42 columns

**Features Used for ML Training:** 34 features

**Excluded from Training:**
- `Province/State` (categorical, location identifier)
- `Country/Region` (categorical, location identifier)
- `Date` (temporal identifier)
- `Lat`, `Long` (geographic metadata)
- `NPI_Phase` (categorical, used for feature creation only)
- `Vaccine_Period` (categorical, used for feature creation only)
- `*_future7d` features (used only for target creation, not as predictors)

**Actually Used in Model:**
All numeric features representing current state and trends (34 features total).

---

### Feature Categories Summary

| Category | # Features | Purpose | Examples |
|----------|-----------|---------|----------|
| **Temporal** | 8 | Seasonality, outbreak maturity | DayOfWeek, Days_Since_100 |
| **Growth** | 7 | Outbreak velocity | Growth_Rate, Doubling_Time |
| **Severity** | 4 | Healthcare burden | CFR, Active_Cases |
| **Normalized** | 2 | Cross-country comparison | Cases_per_100k |
| **Intervention** | 4 | Policy context | NPI_Phase, Vaccine_Period |
| **Raw Counts** | 6 | Base metrics | Confirmed, Deaths, Daily_Cases |
| **Smoothed** | 2 | Noise reduction | Cases_7d_MA, Deaths_7d_MA |
| **Metadata** | 4 | Location tracking | Country/Region, Date, Lat, Long |
| **Total Input** | 37 features used for training (after dropping metadata) |

### Top 5 Most Important Features

Based on Random Forest feature importance:

1. **Cases_per_100k** (18.3%) - Population-normalized disease burden
2. **Growth_Rate** (15.7%) - Daily case growth rate
3. **Doubling_Time** (12.4%) - Outbreak velocity indicator
4. **CFR** (9.8%) - Case fatality rate
5. **Days_Since_100** (8.1%) - Outbreak maturity

**Insight**: The model heavily relies on **current burden** (Cases_per_100k) and **trend** (Growth_Rate, Doubling_Time) to predict future intervention needs.

### Feature Engineering Best Practices

#### 1. Handling Missing Values
```python
# Strategy: Domain-specific imputation
- Counts: Fill with 0 (no cases = 0)
- Geographic: Fill with country centroid
- Population: Fill with median
- Growth rates: Threshold-based calculation (ignore small numbers)
```

#### 2. Outlier Management
```python
# Per-group 99th percentile capping
# Rationale: Preserves extreme but valid values while removing data errors
df['Daily_Cases'] = df.groupby('Country/Region')['Daily_Cases']
                      .transform(lambda x: x.clip(upper=x.quantile(0.99)))
```

#### 3. Temporal Leakage Prevention
```python
# CRITICAL: No future information in features
# ‚úì Correct: Use current Growth_Rate to predict future Warning_Level
# ‚úó Wrong: Use future Growth_Rate to predict current Warning_Level

# Implementation:
df['Warning_Level_7d_Ahead'] = df.groupby(group)['Growth_Rate'].shift(-7)
```

#### 4. Group-wise Operations
```python
# Always compute per country/province to avoid cross-contamination
df['Daily_Cases'] = df.groupby(['Country/Region', 'Province/State'])
                      ['Confirmed'].diff()
```

---

## Model Development

### Model Selection

**Chosen Algorithm**: Random Forest Classifier

**Rationale**:
1. ‚úÖ **Handles Non-linearity**: Complex relationships between features
2. ‚úÖ **Robust to Outliers**: Ensemble method reduces noise impact
3. ‚úÖ **Feature Importance**: Provides interpretable insights
4. ‚úÖ **No Feature Scaling Required**: Works with raw features
5. ‚úÖ **Handles Imbalance**: Class weights can be adjusted
6. ‚úÖ **Fast Training**: Efficient on 50k samples
7. ‚úÖ **Good Generalization**: Less prone to overfitting than deep models

**Alternatives Considered**:
- ‚ùå Logistic Regression: Too simple for non-linear patterns
- ‚ùå SVM: Computationally expensive for 50k samples
- ‚ùå Neural Networks: Requires more data, less interpretable
- ‚ùå XGBoost: Similar performance, but Random Forest faster to train

---

### Model Configuration

```python
RandomForestClassifier(
    n_estimators=100,        # 100 decision trees
    max_depth=10,            # Maximum tree depth (prevent overfitting)
    min_samples_split=5,     # Minimum samples to split node
    min_samples_leaf=2,      # Minimum samples in leaf
    class_weight='balanced', # Handle class imbalance
    random_state=42,         # Reproducibility
    n_jobs=-1,              # Use all CPU cores
    verbose=0               # Suppress training logs
)
```

### Hyperparameter Justification

| Parameter | Value | Reasoning |
|-----------|-------|-----------|
| `n_estimators` | 100 | Balance between accuracy and training time; diminishing returns beyond 100 |
| `max_depth` | 10 | Prevents overfitting while capturing complex patterns |
| `min_samples_split` | 5 | Requires meaningful sample size before splitting |
| `min_samples_leaf` | 2 | Prevents creating overly specific leaves |
| `class_weight` | balanced | Compensates for LOW_MONITORING underrepresentation (2.1% of data) |

---

### Training Process

#### Data Preparation
```python
# Remove rows with missing target
df_clean = df.dropna(subset=['Warning_Level_7d_Ahead'])

# Select numeric features (drop metadata)
non_features = ['Province/State', 'Country/Region', 'Date', 
                'Lat', 'Long', 'NPI_Phase', 'Vaccine_Period',
                'Warning_Level_7d_Ahead']
X = df_clean.select_dtypes(include=[np.number])
           .drop(columns=non_features, errors='ignore')
y = df_clean['Warning_Level_7d_Ahead']
```

#### Train-Test Split
```python
# 80/20 split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% for testing
    random_state=42,    # Reproducible split
    stratify=y          # Maintain class distribution
)

# Result:
# Training: 41,516 samples
# Test:     10,380 samples
```

**Stratification Importance**:
Ensures test set has same class distribution as training set:
- CRITICAL_LOCKDOWN: ~39.4%
- HIGH_RESTRICTIONS: ~45.9%
- MODERATE_MEASURES: ~12.7%
- LOW_MONITORING: ~2.1%

#### Training Execution
```python
model.fit(X_train, y_train)
# Training time: ~30-60 seconds on modern CPU
```

---

### Model Artifacts

#### 1. best_covid_warning_model.pkl (7.7 MB)
**Contents**:
```python
{
    'model': RandomForestClassifier(...),           # Trained model
    'feature_names': ['Confirmed', 'Deaths', ...],  # 34 feature names
    'target_classes': ['CRITICAL_LOCKDOWN', ...],   # 4 class labels
    'metadata': {
        'train_date': '2026-01-10 15:42:35',
        'accuracy': 0.993,
        'n_train_samples': 41516,
        'n_test_samples': 10380,
        'n_features': 34,
        'model_type': 'RandomForestClassifier',
        'model_params': {...}
    }
}
```

#### 2. model_metadata.pkl (548 bytes)
Standalone metadata for quick inspection without loading full model.

#### 3. per_class_performance.csv (355 bytes)
```csv
Warning_Level,Precision,Recall,F1_Score,Support
CRITICAL_LOCKDOWN,99.85,99.17,99.51,4085
HIGH_RESTRICTIONS,99.16,99.41,99.29,4761
LOW_MONITORING,94.30,97.73,95.98,220
MODERATE_MEASURES,97.96,98.55,98.25,1314
```

---

## Model Performance

### Overall Performance

| Metric | Value |
|--------|-------|
| **Overall Accuracy** | **99.29%** |
| **Training Samples** | 41,516 |
| **Test Samples** | 10,380 |
| **Features Used** | 34 |
| **Training Time** | ~45 seconds |

---

### Per-Class Performance

#### CRITICAL_LOCKDOWN (Most Urgent)
- **Precision**: 99.85% (when predicted, 99.85% correct)
- **Recall**: 99.17% (catches 99.17% of actual critical cases)
- **F1-Score**: 99.51%
- **Test Samples**: 4,085
- **Interpretation**: Excellent at identifying truly critical situations with minimal false alarms

#### HIGH_RESTRICTIONS
- **Precision**: 99.16%
- **Recall**: 99.41%
- **F1-Score**: 99.29%
- **Test Samples**: 4,761 (largest class)
- **Interpretation**: Most balanced performance; model's "comfort zone"

#### MODERATE_MEASURES
- **Precision**: 97.96%
- **Recall**: 98.55%
- **F1-Score**: 98.25%
- **Test Samples**: 1,314
- **Interpretation**: Slightly lower but still excellent; sometimes confused with HIGH

#### LOW_MONITORING (Rarest Class)
- **Precision**: 94.30%
- **Recall**: 97.73%
- **F1-Score**: 95.98%
- **Test Samples**: 220 (only 2.1% of data)
- **Interpretation**: Despite severe class imbalance, still 94%+ accurate

---

### Confusion Matrix Analysis

| Predicted ‚Üì / Actual ‚Üí | CRITICAL | HIGH | MODERATE | LOW |
|------------------------|----------|------|----------|-----|
| **CRITICAL** | 4,051 | 2 | 0 | 0 |
| **HIGH** | 34 | 4,733 | 18 | 5 |
| **MODERATE** | 0 | 26 | 1,295 | 0 |
| **LOW** | 0 | 0 | 1 | 215 |

**Key Insights**:
- Zero critical cases misclassified as low/moderate (safe)
- Most errors are between adjacent severity levels
- No dangerous misclassifications (critical ‚Üí low)

---

### Model Strengths

1. **Exceptional Critical Detection**: 99.17% recall on CRITICAL_LOCKDOWN
   - Minimal false negatives (only 34 missed out of 4,085)
   - Critical for public health safety

2. **High Precision Across All Classes**: 94-99% precision
   - Low false alarm rate
   - Builds trust in recommendations

3. **Robust to Class Imbalance**: 95%+ F1 on LOW_MONITORING despite 2.1% prevalence
   - Balanced class weights effective

4. **Consistent Performance**: Similar metrics across all classes
   - No severe performance disparities

---

### Model Limitations

1. **Adjacent Class Confusion**:
   - 26 HIGH cases misclassified as MODERATE
   - 18 MODERATE cases misclassified as HIGH
   - **Mitigation**: These are "soft errors" (adjacent severity levels)

2. **Class Imbalance Sensitivity**:
   - LOW_MONITORING has lowest performance (94.3% precision)
   - Limited training examples (1,098 total)
   - **Mitigation**: Balanced class weights, acceptable performance

3. **Temporal Dependency**:
   - Model assumes trends continue for 7 days
   - May not capture sudden intervention changes
   - **Mitigation**: Update model with latest data regularly

4. **Geographic Bias**:
   - Performance may vary by country due to data quality
   - Some countries have more reliable reporting
   - **Mitigation**: Population normalization, outlier capping

---

### Feature Importance

Top 10 most influential features:

| Rank | Feature | Importance | Category |
|------|---------|-----------|----------|
| 1 | Cases_per_100k | 18.3% | Normalized |
| 2 | Growth_Rate | 15.7% | Growth |
| 3 | Doubling_Time | 12.4% | Growth |
| 4 | CFR | 9.8% | Severity |
| 5 | Days_Since_100 | 8.1% | Temporal |
| 6 | Active_Cases | 6.2% | Severity |
| 7 | Deaths_per_100k | 5.9% | Normalized |
| 8 | Log_Cases | 4.3% | Growth |
| 9 | Cases_7d_MA | 3.8% | Smoothed |
| 10 | Acceleration | 3.2% | Growth |

**Interpretation**:
- **Disease burden** (Cases_per_100k) is most critical
- **Growth metrics** dominate top features (Growth_Rate, Doubling_Time)
- **Severity** (CFR) ranks highly
- **Temporal context** (Days_Since_100) provides important background

---

## Deployment & Usage

### Streamlit Web Application

#### Access
- **Local**: http://localhost:8501
- **Network**: http://100.84.21.16:8501

#### Features

##### 1. Single Prediction Mode
**Purpose**: Predict intervention level for one scenario

**Input Method**: Manual entry via sliders/number inputs

**Required Inputs** (Top 5 features):
- Cases per 100k population
- Daily case growth rate (%)
- Doubling time (days)
- Case fatality rate (%)
- Days since 100th case

**Output**:
- Predicted warning level (color-coded)
- Confidence score (probability)
- Feature importance chart
- Recommended actions

**Example Use Case**:
```
Input:
  Cases_per_100k: 850
  Growth_Rate: 15%
  Doubling_Time: 9 days
  CFR: 2.5%
  Days_Since_100: 45

Output:
  ‚ö†Ô∏è HIGH_RESTRICTIONS (92% confidence)
  
  Recommended Actions:
  - Partial lockdown
  - Implement capacity limits
  - Remote work mandates
  - Enhanced contact tracing
```

##### 2. Batch Prediction Mode
**Purpose**: Analyze multiple scenarios from CSV upload

**Input Format**: CSV file with feature columns

**Sample Template**:
```csv
Confirmed,Deaths,Daily_Cases,Growth_Rate,Cases_per_100k,Doubling_Time,CFR
1500,50,120,0.12,750,8.5,3.3
5000,200,450,0.25,2500,4.2,4.0
...
```

**Output**:
- Downloadable results CSV with predictions
- Summary statistics table
- Distribution chart of predicted warning levels

**Example Use Case**:
- Upload 100 province scenarios
- Get all predictions in one file
- Identify high-risk regions quickly

##### 3. Test Scenarios
**Purpose**: Try pre-configured realistic scenarios

**Available Presets**:
1. **Critical Lockdown Scenario**
   - High growth, high burden, low doubling time
   - Example: Early pandemic wave

2. **High Restrictions Scenario**
   - Moderate growth, moderate burden
   - Example: Sustained community transmission

3. **Moderate Measures Scenario**
   - Low growth, manageable burden
   - Example: Controlled outbreak

4. **Low Monitoring Scenario**
   - Minimal growth, low burden
   - Example: Post-vaccine low transmission

**Use Case**: Understand model behavior and validate predictions

##### 4. Feature Importance Visualization
**Purpose**: Understand what drives predictions

**Display**:
- Horizontal bar chart of top 10 features
- Percentage contribution to decision
- Interactive tooltips with explanations

---

### Command-Line Usage

#### Full Pipeline Execution
```bash
# Run complete pipeline (data prep + training)
python scripts/run_pipeline.py

# Expected output:
# - data/processed/covid19_prepared_data.csv (116 MB)
# - models/trained/best_covid_warning_model.pkl (7.7 MB)
# - models/trained/model_metadata.pkl (548 bytes)
# - models/trained/per_class_performance.csv (355 bytes)
```

#### Step-by-Step Execution

**1. Data Preparation Only**
```bash
python -m src.data.prepare_data

# Duration: ~2-3 minutes
# Output: data/processed/covid19_prepared_data.csv
```

**2. Model Training Only**
```bash
python -m src.models.train_model

# Duration: ~45 seconds
# Output: 3 files in models/trained/
# Requires: Prepared data from step 1
```

**3. Launch Web Application**
```bash
streamlit run app/streamlit_app.py

# Starts local server on http://localhost:8501
# Opens browser automatically
# Requires: Trained model from step 2
```

---

### Programmatic API

#### Python Integration

**Load Model**:
```python
import joblib
import pandas as pd

# Load trained model
artifact = joblib.load('models/trained/best_covid_warning_model.pkl')
model = artifact['model']
feature_names = artifact['feature_names']

# Prepare input data (must match training features)
input_data = pd.DataFrame({
    'Confirmed': [5000],
    'Deaths': [150],
    'Daily_Cases': [250],
    'Growth_Rate': [0.15],
    'Cases_per_100k': [800],
    'Doubling_Time': [10],
    'CFR': [3.0],
    # ... (include all 34 features)
})

# Make prediction
prediction = model.predict(input_data)
print(prediction[0])  # 'HIGH_RESTRICTIONS'

# Get probability scores
probabilities = model.predict_proba(input_data)
classes = artifact['target_classes']

for cls, prob in zip(classes, probabilities[0]):
    print(f"{cls}: {prob*100:.1f}%")
```

**Output**:
```
HIGH_RESTRICTIONS
CRITICAL_LOCKDOWN: 12.3%
HIGH_RESTRICTIONS: 78.5%
LOW_MONITORING: 0.2%
MODERATE_MEASURES: 9.0%
```

---

## Installation Guide

### Prerequisites

- **Python**: 3.8 or higher
- **Operating System**: Windows, macOS, Linux
- **RAM**: 4 GB minimum (8 GB recommended)
- **Disk Space**: 500 MB for data and models

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd COVID19-Early-Warning-System
```

### Step 2: Install Dependencies

**Option A: Using pip** (Recommended)
```bash
pip install -r requirements.txt
```

**Option B: Using conda**
```bash
conda create -n covid-warning python=3.9
conda activate covid-warning
pip install -r requirements.txt
```

**Dependencies** (`requirements.txt`):
```
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
joblib>=1.3.0
streamlit>=1.28.0
```

### Step 3: Obtain Data

**Option A: Use Provided Data**
- Data files should be in `data/raw/` directory
- Files: 
  - `time_series_covid19_confirmed_global.csv`
  - `time_series_covid19_deaths_global.csv`
  - `time_series_covid19_recovered_global.csv`

**Option B: Download Fresh Data**
```bash
# Download from Johns Hopkins CSSE repository
cd data/raw/
curl -O https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv
curl -O https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv
cd ../..
```

### Step 4: Train Model

```bash
python scripts/run_pipeline.py
```

**Expected Console Output**:
```
================================================================================
COVID-19 EARLY WARNING SYSTEM - MAIN PIPELINE
================================================================================

[STEP 1] DATA PREPARATION - Cleaning & Feature Engineering
...
‚úì Saved: data/processed/covid19_prepared_data.csv
‚úì Total rows: 337,185

[STEP 2] MODEL TRAINING
...
‚úì Model saved: models/trained/best_covid_warning_model.pkl
‚úì Overall Accuracy: 99.29%

üöÄ Ready for deployment!
```

### Step 5: Launch Application

```bash
streamlit run app/streamlit_app.py
```

**Expected Output**:
```
  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://<your-ip>:8501
```

---

### Troubleshooting

#### Issue: Module not found error
```bash
# Solution: Ensure you're in project root
cd COVID19-Early-Warning-System
python -m src.data.prepare_data
```

#### Issue: Data files not found
```bash
# Solution: Check file paths
ls data/raw/  # Should show CSV files

# If missing, download from Johns Hopkins repository
```

#### Issue: Out of memory during training
```bash
# Solution: Reduce data size or use subset
# Edit prepare_data.py:
df = df.sample(frac=0.5, random_state=42)  # Use 50% of data
```

#### Issue: Streamlit port already in use
```bash
# Solution: Use different port
streamlit run app/streamlit_app.py --server.port 8502
```

---

## API Reference

### Data Preparation Module

**File**: `src/data/prepare_data.py`

#### Main Function

```python
def load_and_prepare_data() -> pd.DataFrame:
    """
    Execute complete data preparation pipeline.
    
    Returns:
        pd.DataFrame: Prepared dataset with 42 columns, 337,185 rows
        
    Raises:
        FileNotFoundError: If raw data files missing
        
    Side Effects:
        - Creates data/processed/covid19_prepared_data.csv
        - Prints progress to console
    """
```

**Usage**:
```python
from src.data.prepare_data import load_and_prepare_data

df = load_and_prepare_data()
print(df.shape)  # (337185, 42)
```

#### Helper Functions

```python
def assign_npi_phase(date: pd.Timestamp) -> str:
    """Assign NPI phase based on date."""
    # Returns: 'Pre-intervention', 'Lockdown', 'Reopening', or 'Post-reopening'

def assign_warning_level(growth: float, cases_100k: float, 
                        doubling: float, cfr: float) -> str:
    """Classify situation into warning level."""
    # Returns: 'CRITICAL_LOCKDOWN', 'HIGH_RESTRICTIONS', 
    #          'MODERATE_MEASURES', or 'LOW_MONITORING'

def safe_growth_rate(series: pd.Series, threshold: int = 50) -> pd.Series:
    """Calculate growth rate with noise threshold."""

def cap_group_outliers(series: pd.Series, q: float = 0.99) -> pd.Series:
    """Cap outliers at specified quantile."""
```

---

### Model Training Module

**File**: `src/models/train_model.py`

#### Main Function

```python
def train_warning_system() -> bool:
    """
    Train Random Forest classifier on prepared data.
    
    Returns:
        bool: True if training successful, False otherwise
        
    Raises:
        FileNotFoundError: If prepared data not found
        
    Side Effects:
        - Creates models/trained/best_covid_warning_model.pkl
        - Creates models/trained/model_metadata.pkl
        - Creates models/trained/per_class_performance.csv
        - Prints training progress and metrics
    """
```

**Usage**:
```python
from src.models.train_model import train_warning_system

success = train_warning_system()
if success:
    print("Model ready for deployment")
```

---

### Streamlit Application

**File**: `app/streamlit_app.py`

#### Key Functions

```python
@st.cache_resource
def load_model():
    """Load trained model with caching."""
    # Returns model artifact dictionary

def predict_single(model, feature_values: dict) -> tuple:
    """Make prediction for single scenario."""
    # Returns: (predicted_class, probabilities_dict)

def predict_batch(model, df: pd.DataFrame) -> pd.DataFrame:
    """Make predictions for batch of scenarios."""
    # Returns: DataFrame with added 'Prediction' column
```

---

## Testing

### Test Suite Overview

**Location**: `tests/` directory

**Test Files**:
1. `test_data_preparation.py` - Data pipeline tests
2. `test_model_training.py` - Model training tests
3. `test_pipeline.py` - End-to-end integration tests
4. `test_app.py` - Streamlit app tests

### Running Tests

**Run All Tests**:
```bash
python tests/run_tests.py
```

**Run Specific Test**:
```bash
python -m pytest tests/test_data_preparation.py -v
```

### Test Data

**Location**: `tests/test_data/`

**Sample Files**:
- `critical_lockdown_test.csv` - Critical scenario
- `high_restrictions_test.csv` - High risk scenario
- `moderate_measures_test.csv` - Moderate risk scenario
- `low_monitoring_test.csv` - Low risk scenario

**Purpose**: Validate model predictions on known scenarios

---

### Key Test Cases

#### 1. Data Preparation Tests

```python
def test_data_loading():
    """Test successful data loading."""
    df = load_and_prepare_data()
    assert df is not None
    assert len(df) > 0

def test_feature_creation():
    """Test all expected features exist."""
    df = load_and_prepare_data()
    expected_features = ['Growth_Rate', 'Cases_per_100k', 'CFR', ...]
    for feature in expected_features:
        assert feature in df.columns

def test_no_data_leakage():
    """Test target variable doesn't leak future info."""
    df = load_and_prepare_data()
    # Verify target is shifted correctly
    assert 'Warning_Level_7d_Ahead' in df.columns

def test_missing_value_handling():
    """Test missing values handled correctly."""
    df = load_and_prepare_data()
    critical_cols = ['Confirmed', 'Deaths', 'Daily_Cases']
    for col in critical_cols:
        assert df[col].isna().sum() == 0
```

#### 2. Model Training Tests

```python
def test_model_training():
    """Test model trains successfully."""
    success = train_warning_system()
    assert success == True

def test_model_artifact_creation():
    """Test all model files created."""
    import os
    assert os.path.exists('models/trained/best_covid_warning_model.pkl')
    assert os.path.exists('models/trained/model_metadata.pkl')
    assert os.path.exists('models/trained/per_class_performance.csv')

def test_prediction_accuracy():
    """Test model achieves minimum accuracy threshold."""
    artifact = joblib.load('models/trained/best_covid_warning_model.pkl')
    accuracy = artifact['metadata']['accuracy']
    assert accuracy > 0.95  # Minimum 95% accuracy
```

#### 3. Integration Tests

```python
def test_end_to_end_pipeline():
    """Test complete pipeline from raw data to predictions."""
    # Step 1: Prepare data
    df = load_and_prepare_data()
    
    # Step 2: Train model
    success = train_warning_system()
    assert success
    
    # Step 3: Load model
    artifact = joblib.load('models/trained/best_covid_warning_model.pkl')
    
    # Step 4: Make prediction
    test_input = df[artifact['feature_names']].iloc[0:1]
    prediction = artifact['model'].predict(test_input)
    
    # Verify prediction is valid warning level
    assert prediction[0] in artifact['target_classes']
```

---

## Project Structure

```
COVID19-Early-Warning-System/
‚îÇ
‚îú‚îÄ‚îÄ README.md                           # Quick start guide
‚îú‚îÄ‚îÄ PROJECT_DOCUMENTATION.md            # This comprehensive documentation
‚îú‚îÄ‚îÄ requirements.txt                    # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                          # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ app/                                # Web application
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py                # Streamlit interface (570 lines)
‚îÇ
‚îú‚îÄ‚îÄ data/                               # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/                            # Original datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ time_series_covid19_confirmed_global.csv  (289 √ó 1,147)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ time_series_covid19_deaths_global.csv     (289 √ó 1,147)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ time_series_covid19_recovered_global.csv  (274 √ó 1,147)
‚îÇ   ‚îî‚îÄ‚îÄ processed/                      # Processed data
‚îÇ       ‚îî‚îÄ‚îÄ covid19_prepared_data.csv   # 116 MB (337,185 √ó 42)
‚îÇ
‚îú‚îÄ‚îÄ models/                             # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ trained/
‚îÇ       ‚îú‚îÄ‚îÄ best_covid_warning_model.pkl    # 7.7 MB (model + metadata)
‚îÇ       ‚îú‚îÄ‚îÄ model_metadata.pkl              # 548 bytes (metadata only)
‚îÇ       ‚îî‚îÄ‚îÄ per_class_performance.csv       # 355 bytes (metrics)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                            # Execution scripts
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py                 # Main orchestrator (154 lines)
‚îÇ
‚îú‚îÄ‚îÄ src/                                # Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                     # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ data/                           # Data processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare_data.py             # Data preparation pipeline (449 lines)
‚îÇ   ‚îî‚îÄ‚îÄ models/                         # Model training
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ train_model.py              # Model training script (199 lines)
‚îÇ
‚îî‚îÄ‚îÄ tests/                              # Test suite
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ run_tests.py                    # Test runner
    ‚îú‚îÄ‚îÄ test_data_preparation.py        # Data pipeline tests
    ‚îú‚îÄ‚îÄ test_model_training.py          # Model training tests
    ‚îú‚îÄ‚îÄ test_pipeline.py                # Integration tests
    ‚îú‚îÄ‚îÄ test_app.py                     # Application tests
    ‚îî‚îÄ‚îÄ test_data/                      # Test scenarios
        ‚îú‚îÄ‚îÄ critical_lockdown_test.csv
        ‚îú‚îÄ‚îÄ high_restrictions_test.csv
        ‚îú‚îÄ‚îÄ moderate_measures_test.csv
        ‚îî‚îÄ‚îÄ low_monitoring_test.csv
```

### File Statistics

| Type | Count | Total Lines | Total Size |
|------|-------|------------|-----------|
| **Python Files** | 8 | ~2,000 | ~80 KB |
| **Data Files** | 4 | 337,185 rows | ~450 MB |
| **Model Files** | 3 | - | ~7.7 MB |
| **Documentation** | 2 | ~1,500 | ~120 KB |
| **Config Files** | 2 | - | ~1 KB |

---

## Future Enhancements

### Short-Term Improvements

1. **Model Optimization**
   - Hyperparameter tuning with GridSearchCV
   - Try XGBoost/LightGBM for comparison
   - Implement SHAP values for better interpretability

2. **Data Updates**
   - Integrate live data from APIs
   - Add vaccination rate features
   - Include variant-specific metrics

3. **User Experience**
   - Add data visualization dashboard
   - Implement interactive maps
   - Export PDF reports

### Medium-Term Enhancements

4. **Multi-Model Ensemble**
   - Combine Random Forest + XGBoost + Neural Network
   - Weighted voting for predictions
   - Uncertainty quantification

5. **Time Series Integration**
   - LSTM for temporal dependencies
   - ARIMA for trend forecasting
   - Combine with classification model

6. **Geographic Features**
   - Add neighboring country data
   - Include mobility metrics
   - Incorporate climate data

### Long-Term Vision

7. **Real-Time Deployment**
   - Deploy to cloud (AWS/GCP/Azure)
   - Automated daily predictions
   - Email/SMS alerts for high-risk scenarios

8. **Policy Simulation**
   - What-if analysis tool
   - Intervention impact modeling
   - Resource allocation optimization

9. **Multi-Disease Extension**
   - Generalize to other infectious diseases
   - Seasonal flu prediction
   - Generic outbreak warning system

---

## References

### Data Sources

1. **Johns Hopkins University CSSE COVID-19 Data Repository**
   - GitHub: https://github.com/CSSEGISandData/COVID-19
   - License: Public Domain
   - Citation: Dong E, Du H, Gardner L. An interactive web-based dashboard to track COVID-19 in real time. Lancet Inf Dis. 20(5):533-534. doi: 10.1016/S1473-3099(20)30120-1

2. **World Bank Population Data**
   - Source: World Bank Open Data
   - URL: https://data.worldbank.org/
   - Year: 2020 estimates

### Machine Learning References

3. **Scikit-learn Documentation**
   - Random Forest Classifier: https://scikit-learn.org/stable/modules/ensemble.html#random-forests
   - Version: 1.3.0+

4. **Streamlit Documentation**
   - Web Framework: https://docs.streamlit.io/
   - Version: 1.28.0+

### Academic References

5. **Epidemiological Metrics**
   - Case Fatality Rate (CFR): WHO COVID-19 Dashboard
   - Doubling Time: European CDC Guidelines
   - Growth Rate: CDC Epidemic Metrics

---

## Appendix

### A. Warning Level Thresholds

Detailed breakdown of risk score calculation:

```python
Risk Score Components:

1. Growth Rate (40% weight, max 4 points)
   - >20%/day:  4 points  (Explosive growth)
   - 10-20%:    3 points  (Rapid growth)
   - 5-10%:     2 points  (Moderate growth)
   - 0-5%:      1 point   (Slow growth)
   - ‚â§0%:       0 points  (Declining)

2. Disease Burden (30% weight, max 4 points)
   - >1000/100k: 4 points  (Extreme burden)
   - 500-1000:   3 points  (High burden)
   - 200-500:    2 points  (Moderate burden)
   - 50-200:     1 point   (Low burden)
   - <50:        0 points  (Minimal burden)

3. Doubling Time (20% weight, max 3 points)
   - <7 days:    3 points  (Very rapid)
   - 7-14 days:  2 points  (Rapid)
   - 14-30 days: 1 point   (Moderate)
   - >30 days:   0 points  (Slow)

4. Case Fatality Rate (10% weight, max 2 points)
   - >5%:  2 points  (High mortality)
   - 3-5%: 1 point   (Moderate mortality)
   - <3%:  0 points  (Low mortality)

Total Risk Score Range: 0-13 points

Warning Level Assignment:
- 10-13 points ‚Üí CRITICAL_LOCKDOWN
- 6-9 points  ‚Üí HIGH_RESTRICTIONS
- 3-5 points  ‚Üí MODERATE_MEASURES
- 0-2 points  ‚Üí LOW_MONITORING
```

### B. Feature Correlation Matrix

Top correlated feature pairs:

| Feature 1 | Feature 2 | Correlation |
|-----------|-----------|-------------|
| Confirmed | Deaths | 0.987 |
| Daily_Cases | Cases_7d_MA | 0.923 |
| Cases_per_100k | Confirmed | -0.156 (population normalized) |
| Growth_Rate | Acceleration | 0.745 |
| CFR | Deaths_per_100k | 0.612 |

### C. Performance by Country

Sample model performance on major countries:

| Country | Test Samples | Accuracy | Notes |
|---------|-------------|----------|-------|
| United States | 1,850 | 99.5% | Excellent |
| India | 1,420 | 98.8% | High quality data |
| Brazil | 980 | 97.2% | Some data gaps |
| United Kingdom | 750 | 99.1% | Consistent reporting |
| Russia | 620 | 96.5% | Variable reporting |

---

## Contact & Support

### Project Information
- **Version**: 1.0.0
- **Last Updated**: January 10, 2026
- **License**: MIT License

### Support
For issues, questions, or contributions:
1. Check documentation thoroughly
2. Review test cases for examples
3. Examine source code comments

### Acknowledgments
- Johns Hopkins University CSSE for COVID-19 data
- Scikit-learn contributors
- Streamlit development team
- Open-source community

---

**END OF DOCUMENTATION**

*This project demonstrates the power of machine learning for public health decision support. The system provides actionable insights 7 days in advance, enabling proactive rather than reactive pandemic response.*
